{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL\n",
    "BASE_URL = \"https://www.tacchini.it/en/\"\n",
    "COLLECTION_URL = \"https://www.tacchini.it/en/collections/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_soup(url):\n",
    "    \"\"\"Fetch HTML content and return a BeautifulSoup object.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "    else:\n",
    "        print(f\"Failed to fetch {url}, Status Code: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_products(soup):\n",
    "    \"\"\"Extract product links and basic info from the collection page.\"\"\"\n",
    "    products = []\n",
    "    product_divs = soup.find_all('div', class_='titolo-prodotto')\n",
    "\n",
    "    for product in product_divs:\n",
    "        a_tag = product.find('a')\n",
    "        if a_tag:\n",
    "            name = a_tag.contents[0].strip()\n",
    "            designer_tag = a_tag.find('span', class_='designer')\n",
    "            designer_name = designer_tag.text.strip() if designer_tag else \"Unknown\"\n",
    "            product_link = BASE_URL + a_tag['href']\n",
    "            \n",
    "            products.append({\n",
    "                \"name\": name,\n",
    "                \"designer\": designer_name,\n",
    "                \"link\": product_link\n",
    "            })\n",
    "\n",
    "    return products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_product_details(product):\n",
    "    \"\"\"Scrape individual product details from its page.\"\"\"\n",
    "    soup = fetch_soup(product[\"link\"])\n",
    "    if not soup:\n",
    "        return None\n",
    "    \n",
    "    # Extract description\n",
    "    description_div = soup.find('div', class_='text-normal visibile')\n",
    "    description = description_div.p.text.strip() if description_div and description_div.p else \"No description available\"\n",
    "    \n",
    "    # Extract dimensions\n",
    "    dimensions_div = soup.find_all('div', class_='testo-tecnico')\n",
    "    dimensions = [dim.p.text.strip() for dim in dimensions_div if dim.p] if dimensions_div else []\n",
    "    \n",
    "    # Extract materials\n",
    "    materials_div = soup.find('div', class_='left-column-a')\n",
    "    materials = [mat.text.strip() for mat in materials_div.find_all('div', class_='text-a')] if materials_div else []\n",
    "    \n",
    "    # Return updated product details\n",
    "    product.update({\n",
    "        \"description\": description,\n",
    "        \"dimensions\": dimensions,\n",
    "        \"materials\": materials\n",
    "    })\n",
    "    return product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_product_details(product):\n",
    "    \"\"\"Scrape individual product details from its page.\"\"\"\n",
    "    soup = fetch_soup(product[\"link\"])\n",
    "    if not soup:\n",
    "        return None\n",
    "    \n",
    "    # Extract description: Get text from all child elements\n",
    "    description = \"No description available\"\n",
    "    description_div = soup.find('div', class_=re.compile(r'text-normal\\s+visibile'))  # Regex match\n",
    "\n",
    "    if description_div:\n",
    "        paragraphs = [p.get_text(strip=True) for p in description_div.find_all(['p', 'span', 'div']) if p.get_text(strip=True)]\n",
    "        description = \" \".join(paragraphs) if paragraphs else \"No description available\"\n",
    "\n",
    "    # Backup: Try meta description if div extraction fails\n",
    "    if description == \"No description available\":\n",
    "        meta_desc = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "        if meta_desc and \"content\" in meta_desc.attrs:\n",
    "            description = meta_desc[\"content\"]\n",
    "\n",
    "    # Extract dimensions\n",
    "    dimensions_div = soup.find_all('div', class_='testo-tecnico')\n",
    "    dimensions = [dim.get_text(strip=True) for dim in dimensions_div] if dimensions_div else []\n",
    "    \n",
    "    # Extract materials\n",
    "    materials_div = soup.find('div', class_='left-column-a')\n",
    "    materials = [mat.get_text(strip=True) for mat in materials_div.find_all('div', class_='text-a')] if materials_div else []\n",
    "    \n",
    "    # Return updated product details\n",
    "    product.update({\n",
    "        \"description\": description,\n",
    "        \"dimensions\": dimensions,\n",
    "        \"materials\": materials\n",
    "    })\n",
    "    return product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimental 2\n",
    "def extract_product_details(product):\n",
    "    \"\"\"Scrape individual product details from its page.\"\"\"\n",
    "    soup = fetch_soup(product[\"link\"])\n",
    "    if not soup:\n",
    "        return None\n",
    "    \n",
    "    # Extract description: Get text from all child elements\n",
    "    description = \"No description available\"\n",
    "    description_div = soup.find('div', class_=re.compile(r'text-normal\\s+visibile'))  # Regex match\n",
    "\n",
    "    if description_div:\n",
    "        paragraphs = [p.get_text(strip=True) for p in description_div.find_all(['p', 'span', 'div']) if p.get_text(strip=True)]\n",
    "        description = \" \".join(paragraphs) if paragraphs else \"No description available\"\n",
    "\n",
    "    # Backup: Try meta description if div extraction fails\n",
    "    if description == \"No description available\":\n",
    "        meta_desc = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "        if meta_desc and \"content\" in meta_desc.attrs:\n",
    "            description = meta_desc[\"content\"]  \n",
    "\n",
    "    # Extract image\n",
    "    image_url = \"No image available\"\n",
    "    img_tag = soup.find('img', src=re.compile(r'https://www\\.tacchini\\.it/wp-content/uploads/'))\n",
    "    if img_tag and \"src\" in img_tag.attrs:\n",
    "        image_url = img_tag[\"src\"]\n",
    "\n",
    "    # Extract dimensions\n",
    "    dimensions_div = soup.find_all('div', class_='testo-tecnico')\n",
    "    dimensions = [dim.get_text(strip=True) for dim in dimensions_div] if dimensions_div else []\n",
    "    \n",
    "    # Extract materials\n",
    "    materials_div = soup.find('div', class_='left-column-a')\n",
    "    materials = [mat.get_text(strip=True) for mat in materials_div.find_all('div', class_='text-a')] if materials_div else []\n",
    "    \n",
    "    # Return updated product details\n",
    "    product.update({\n",
    "        \"description\": description,\n",
    "        \"image\": image_url,\n",
    "        \"dimensions\": dimensions,\n",
    "        \"materials\": materials\n",
    "    })\n",
    "    return product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data, filename=\"products.json\"):\n",
    "    \"\"\"Save the extracted data to a JSON file.\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, indent=4, ensure_ascii=False)\n",
    "    print(f\"Scraping complete. Data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution flow.\"\"\"\n",
    "    soup = fetch_soup(COLLECTION_URL)\n",
    "    if not soup:\n",
    "        return\n",
    "\n",
    "    products = extract_products(soup)\n",
    "    product_data = []\n",
    "\n",
    "    for idx, product in enumerate(products, start=1):\n",
    "        print(f\"Scraping product {idx}/{len(products)}: {product['name']}\")  # Progress log\n",
    "        product_details = extract_product_details(product)\n",
    "        if product_details:\n",
    "            product_details[\"id\"] = idx\n",
    "            product_data.append(product_details)\n",
    "\n",
    "    save_to_json(product_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping product 1/146: Solar\n",
      "Scraping product 2/146: Additional System\n",
      "Scraping product 3/146: Le Mura\n",
      "Scraping product 4/146: Victoria\n",
      "Scraping product 5/146: Orsola\n",
      "Scraping product 6/146: Julep\n",
      "Scraping product 7/146: Julep Chaise-longue\n",
      "Scraping product 8/146: Julep Island\n",
      "Scraping product 9/146: Roma Nuvola\n",
      "Scraping product 10/146: Roma\n",
      "Scraping product 11/146: Grand Sesann\n",
      "Scraping product 12/146: Sesann\n",
      "Scraping product 13/146: Face to Face\n",
      "Scraping product 14/146: Sella\n",
      "Scraping product 15/146: Five to Nine\n",
      "Scraping product 16/146: Oliver\n",
      "Scraping product 17/146: Quilt\n",
      "Scraping product 18/146: Lima\n",
      "Scraping product 19/146: Montevideo\n",
      "Scraping product 20/146: Labanca\n",
      "Scraping product 21/146: Sliding\n",
      "Scraping product 22/146: Quadro\n",
      "Scraping product 23/146: Additional System Armchair\n",
      "Scraping product 24/146: Le Mura Armchair\n",
      "Scraping product 25/146: Orsola Armchair\n",
      "Scraping product 26/146: Elephant\n",
      "Scraping product 27/146: Sella Armchair\n",
      "Scraping product 28/146: Roma Armchair\n",
      "Scraping product 29/146: Crystal Armchair\n",
      "Scraping product 30/146: Lagoa\n",
      "Scraping product 31/146: Julep Armchair\n",
      "Scraping product 32/146: Sesann Armchair\n",
      "Scraping product 33/146: Baobab\n",
      "Scraping product 34/146: Pastilles\n",
      "Scraping product 35/146: Pisa\n",
      "Scraping product 36/146: Polar Armchair\n",
      "Scraping product 37/146: Lina\n",
      "Scraping product 38/146: Agnese\n",
      "Scraping product 39/146: Chill-Out Armchair\n",
      "Scraping product 40/146: Lima Armchair\n",
      "Scraping product 41/146: Montevideo Armchair\n",
      "Scraping product 42/146: Reversível\n",
      "Scraping product 43/146: Kelly E\n",
      "Scraping product 44/146: Isola\n",
      "Scraping product 45/146: Glide\n",
      "Scraping product 46/146: Mayfair\n",
      "Scraping product 47/146: Parentesi\n",
      "Scraping product 48/146: Jacket\n",
      "Scraping product 49/146: Moon\n",
      "Scraping product 50/146: Costela\n",
      "Scraping product 51/146: Shelter\n",
      "Scraping product 52/146: Sancarlo\n",
      "Scraping product 53/146: Giulia\n",
      "Scraping product 54/146: SouthBeach\n",
      "Scraping product 55/146: Additional System Ottoman\n",
      "Scraping product 56/146: Le Mura Ottoman\n",
      "Scraping product 57/146: Sesann Ottoman\n",
      "Scraping product 58/146: Torus\n",
      "Scraping product 59/146: Matera\n",
      "Scraping product 60/146: Float\n",
      "Scraping product 61/146: Spin\n",
      "Scraping product 62/146: Pastilles Ottoman\n",
      "Scraping product 63/146: Paola\n",
      "Scraping product 64/146: Dialogo without Armrests\n",
      "Scraping product 65/146: Dialogo with Armrests\n",
      "Scraping product 66/146: Sempronia\n",
      "Scraping product 67/146: Pigreco\n",
      "Scraping product 68/146: Memory Lane\n",
      "Scraping product 69/146: Kelly C Basic\n",
      "Scraping product 70/146: Kelly C\n",
      "Scraping product 71/146: Babela\n",
      "Scraping product 72/146: Montevideo Chair\n",
      "Scraping product 73/146: Eddy\n",
      "Scraping product 74/146: Trono\n",
      "Scraping product 75/146: Fixie\n",
      "Scraping product 76/146: Orbit\n",
      "Scraping product 77/146: Brut\n",
      "Scraping product 78/146: Cassero\n",
      "Scraping product 79/146: Trono Low Table\n",
      "Scraping product 80/146: Altar\n",
      "Scraping product 81/146: Cage\n",
      "Scraping product 82/146: Pluto\n",
      "Scraping product 83/146: Joaquim\n",
      "Scraping product 84/146: Kanji\n",
      "Scraping product 85/146: Dolmen\n",
      "Scraping product 86/146: Mill\n",
      "Scraping product 87/146: Trampolino\n",
      "Scraping product 88/146: Daze\n",
      "Scraping product 89/146: Soap\n",
      "Scraping product 90/146: Pastilles Table\n",
      "Scraping product 91/146: Split\n",
      "Scraping product 92/146: Polar Table\n",
      "Scraping product 93/146: Ledge\n",
      "Scraping product 94/146: Chill-Out Table\n",
      "Scraping product 95/146: Clockwise\n",
      "Scraping product 96/146: Orbit\n",
      "Scraping product 97/146: T–Table\n",
      "Scraping product 98/146: Orpheus\n",
      "Scraping product 99/146: Parker\n",
      "Scraping product 100/146: Togrul\n",
      "Scraping product 101/146: Split\n",
      "Scraping product 102/146: Kelly T\n",
      "Scraping product 103/146: Colombo and 1953\n",
      "Scraping product 104/146: Torii Love\n",
      "Scraping product 105/146: Astral\n",
      "Scraping product 106/146: Serie 500\n",
      "Scraping product 107/146: Chill-Out\n",
      "Scraping product 108/146: Chill-Out High\n",
      "Scraping product 109/146: Polar\n",
      "Scraping product 110/146: Polar Perch\n",
      "Scraping product 111/146: Galleria\n",
      "Scraping product 112/146: Stone\n",
      "Scraping product 113/146: Pigreco The Blue Window\n",
      "Scraping product 114/146: Pigreco Numbered Limited Edition\n",
      "Scraping product 115/146: Dana\n",
      "Scraping product 116/146: Lunar\n",
      "Scraping product 117/146: Doric\n",
      "Scraping product 118/146: Vertical Nest\n",
      "Scraping product 119/146: Mano Light\n",
      "Scraping product 120/146: Fackel\n",
      "Scraping product 121/146: Equinox\n",
      "Scraping product 122/146: Solstice\n",
      "Scraping product 123/146: A.D.A.\n",
      "Scraping product 124/146: Alma\n",
      "Scraping product 125/146: E63\n",
      "Scraping product 126/146: Sophia\n",
      "Scraping product 127/146: Miss Pack\n",
      "Scraping product 128/146: Gunta\n",
      "Scraping product 129/146: Leise\n",
      "Scraping product 130/146: Jacob\n",
      "Scraping product 131/146: Kuschel\n",
      "Scraping product 132/146: Anni\n",
      "Scraping product 133/146: Narciso\n",
      "Scraping product 134/146: Rituale\n",
      "Scraping product 135/146: Aqua Regis\n",
      "Scraping product 136/146: Bubble\n",
      "Scraping product 137/146: Friedl, Lucie and Marlene\n",
      "Scraping product 138/146: Mantiqueira\n",
      "Scraping product 139/146: Pablo & Dora\n",
      "Scraping product 140/146: Pi-Dou\n",
      "Scraping product 141/146: Venus\n",
      "Scraping product 142/146: Tarsia\n",
      "Scraping product 143/146: Stellar\n",
      "Scraping product 144/146: Ancora\n",
      "Scraping product 145/146: Dorian\n",
      "Scraping product 146/146: Soleil\n",
      "Scraping complete. Data saved to products.json\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized JSON saved to normalized_product.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def normalize_json(file_path, output_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    for product in data:\n",
    "        # Normalize dimensions\n",
    "        if 'dimensions' in product and isinstance(product['dimensions'], list):\n",
    "            new_dimensions = []\n",
    "            for dim in product['dimensions']:\n",
    "                dim_values = re.findall(r'\\d+', dim)  # Extract numeric values\n",
    "                if len(dim_values) >= 3:\n",
    "                    dimension_entry = {\n",
    "                        \"code\": dim.split()[0],  # Assuming first word is the code\n",
    "                        \"width\": int(dim_values[0]),\n",
    "                        \"depth\": int(dim_values[1]),\n",
    "                        \"height\": int(dim_values[2])\n",
    "                    }\n",
    "                    if len(dim_values) > 3:\n",
    "                        dimension_entry[\"seat_height\"] = int(dim_values[3])\n",
    "                    new_dimensions.append(dimension_entry)\n",
    "            product['dimensions'] = new_dimensions\n",
    "        \n",
    "        # Normalize materials\n",
    "        if 'materials' in product and isinstance(product['materials'], str):\n",
    "            product['materials'] = [m.strip() for m in product['materials'].split(';')]\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Normalized JSON saved to {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "normalize_json('products.json', 'normalized_product.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def json_to_csv(json_file, csv_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for product in data:\n",
    "        product_base = {\n",
    "            \"ID\": product.get(\"id\"),\n",
    "            \"Name\": product.get(\"name\"),\n",
    "            \"Designer\": product.get(\"designer\"),\n",
    "            \"Link\": product.get(\"link\"),\n",
    "            \"Description\": product.get(\"description\"),\n",
    "            \"Image\": product.get(\"image\"),\n",
    "            \"Materials\": \"; \".join(product.get(\"materials\", [])),  # Join materials into a single string\n",
    "        }\n",
    "\n",
    "        # Handling multiple dimension sets by creating separate rows for each\n",
    "        for dimension in product.get(\"dimensions\", []):\n",
    "            row = product_base.copy()\n",
    "            row.update({\n",
    "                \"Code\": dimension.get(\"code\"),\n",
    "                \"Width\": dimension.get(\"width\"),\n",
    "                \"Depth\": dimension.get(\"depth\"),\n",
    "                \"Height\": dimension.get(\"height\"),\n",
    "                \"Seat Height\": dimension.get(\"seat_height\"),\n",
    "            })\n",
    "            rows.append(row)\n",
    "\n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "\n",
    "# Example usage:\n",
    "json_to_csv(\"normalized_product.json\", \"product.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Archipelago",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
